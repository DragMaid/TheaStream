{% load static %}
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Cambodian Theater</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/vosk-browser@0.0.5/dist/vosk.js"></script>
    <script src="https://unpkg.com/@phosphor-icons/web"></script>
</head>

<body class="bg-zinc-900 text-white h-screen overflow-hidden">
    <style>
        .group:hover .group-hover\:opacity-100 {
            opacity: 1;
        }
    </style>

    <div class="flex h-full">

        <!-- Video Section -->
        <div class="w-2/3 p-6 relative">
            <h1 class="text-3xl font-semibold mb-4 text-white">Cambodian Theater Performance</h1>
            <div class="relative w-full rounded-xl overflow-hidden shadow-lg">
                <video id="theater-video" controls autoplay muted class="w-full h-auto z-10 relative bg-black"
                    style="z-index: 10;">
                    <source src="{{ video_url }}" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>


        <!-- Chat Section -->
        <div class="w-1/3 bg-zinc-800 border-l border-zinc-700 flex flex-col">

            <!-- Header with mic toggle -->
            <div class="flex items-center justify-between p-4 border-b border-zinc-700">
                <h2 class="text-xl font-medium">Ask About the Performance</h2>
                <button id="vosk-toggle-btn" onclick="toggleVosk()"
                    class="p-2 rounded-full bg-zinc-700 hover:bg-zinc-600 transition relative group">
                    <i id="mic-icon" class="ph ph-microphone text-xl text-white"></i>
                    <span
                        class="absolute bottom-full mb-2 left-1/2 -translate-x-1/2 bg-black text-xs text-white px-2 py-1 rounded opacity-0 group-hover:opacity-100 transition">
                        Toggle Mic
                    </span>
                </button>
            </div>


            <!-- Chat History -->
            <div id="chat-history" class="flex-1 overflow-y-auto px-4 py-4 space-y-3">
                <!-- Messages will be dynamically added here -->
            </div>

            <!-- Chat Input -->
            <div class="p-4 border-t border-zinc-700">
                <form id="chat-form" onsubmit="handleSubmit(event)" class="relative">
                    <textarea id="user_message" rows="2"
                        class="w-full resize-none p-3 pr-10 text-sm bg-zinc-700 text-white rounded-xl focus:outline-none focus:ring-2 focus:ring-yellow-400"
                        placeholder="Type your question here..." required></textarea>
                    <button type="submit"
                        class="absolute bottom-3 right-3 text-yellow-400 hover:text-yellow-300 transition">
                        <i class="ph ph-paper-plane-right text-xl"></i>
                    </button>
                </form>
            </div>

        </div>
    </div>

    <script>
        // Handle Enter to send
        document.getElementById('user_message').addEventListener('keydown', function (e) {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                document.getElementById('chat-form').dispatchEvent(new Event('submit'));
            }
        });

        async function handleSubmit(e) {
            e.preventDefault();
            const textarea = document.getElementById("user_message");
            const question = textarea.value.trim();
            if (!question) return;

            addMessage("user", question);
            textarea.value = "";

            const video = document.getElementById('theater-video');
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            const ctx = canvas.getContext('2d');
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const blob = await new Promise(resolve => canvas.toBlob(resolve, 'image/jpeg'));

            const formData = new FormData();
            formData.append('question', question);
            formData.append('screenshot', blob, 'screenshot.jpg');

            addMessage("ai", "Thinking...");

            try {
                const response = await fetch('/ask-ai/', {
                    method: 'POST',
                    body: formData
                });
                const data = await response.json();

                updateLastAIMessage(data.response || "Sorry, I didn't get that.");
            } catch (error) {
                console.error(error);
                updateLastAIMessage("Something went wrong.");
            }
        }

        function addMessage(sender, text) {
            const chat = document.getElementById("chat-history");
            const msg = document.createElement("div");
            msg.className = `flex ${sender === "user" ? "justify-end" : "justify-start"}`;
            msg.innerHTML = `
        <div class="max-w-[80%] px-4 py-2 rounded-xl text-sm ${sender === "user"
                    ? "bg-yellow-400 text-black rounded-br-none"
                    : "bg-zinc-700 text-white rounded-bl-none"
                }">
          ${text}
        </div>`;
            chat.appendChild(msg);
            chat.scrollTop = chat.scrollHeight;
        }

        function updateLastAIMessage(newText) {
            const chat = document.getElementById("chat-history");
            const messages = chat.querySelectorAll(".justify-start");
            if (messages.length > 0) {
                const last = messages[messages.length - 1];
                last.querySelector("div").innerText = newText;
            }
        }

        // --- VOSK Toggle ---
        let voskEnabled = false;
        let recognizer = null;
        let audioContext = null;
        let recognizerNode = null;

        async function toggleVosk() {
            voskEnabled = !voskEnabled;
            const btn = document.getElementById('vosk-toggle-btn');
            const icon = document.getElementById('mic-icon');

            if (voskEnabled) {
                icon.className = "ph ph-microphone-slash text-xl text-red-500 animate-pulse";
                await startVosk();
            } else {
                icon.className = "ph ph-microphone text-xl text-white";
                stopVosk();
            }
        }


        async function startVosk() {
            const model = await Vosk.createModel('/static/models/model.zip');
            recognizer = new model.KaldiRecognizer(16000);
            const call_lines = ["hey alex", "hi alex", "hello alex"];
            const snd = new Audio('/static/audio/listening.mp3');

            recognizer.on("result", (message) => {
                const transcript = message.result.text.trim().toLowerCase();
                if (call_lines.some(v => transcript.includes(v))) {
                    snd.play();
                    snd.currentTime = 0;
                    startRecording();
                }
            });

            const mediaStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    channelCount: 1,
                },
                video: false,
            });

            audioContext = new AudioContext({ sampleRate: 16000 });
            recognizerNode = audioContext.createScriptProcessor(4096, 1, 1);

            recognizerNode.onaudioprocess = (event) => {
                try {
                    recognizer.acceptWaveform(event.inputBuffer);
                } catch (error) {
                    console.error('acceptWaveform failed', error);
                }
            };

            const source = audioContext.createMediaStreamSource(mediaStream);
            source.connect(recognizerNode);
            recognizerNode.connect(audioContext.destination);
        }

        function stopVosk() {
            if (recognizerNode) recognizerNode.disconnect();
            if (audioContext) audioContext.close();
            recognizerNode = null;
            audioContext = null;
            recognizer = null;
        }

        // Voice capture
        let mediaRecorder;
        let audioChunks = [];

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];

            mediaRecorder.ondataavailable = event => {
                audioChunks.push(event.data);
            };

            mediaRecorder.onstop = () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                sendToServer(audioBlob);
                console.log("Sent new audio to server for transcription")
            };

            mediaRecorder.start();
            setTimeout(() => stopRecording(), 8000);
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
        }

        async function sendToServer(audioBlob) {
            const formData = new FormData();
            formData.append("audio", audioBlob, "command.webm");

            const response = await fetch("/api/transcribe", {
                method: "POST",
                body: formData,
            });

            const result = await response.json();
            console.log("Whisper transcript:", result.text);
        }

    </script>
</body>

</html>